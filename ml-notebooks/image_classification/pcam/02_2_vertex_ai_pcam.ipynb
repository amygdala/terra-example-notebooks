{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6e2c084da6b2",
    "tags": []
   },
   "source": [
    "# Using Vertex AI to train an image classification model\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "Run the <a href=\"xxx\"><code>00_pcam_setup.ipynb notebook</code></a> first, before running this one.  You'll need the settings info from that notebook.\n",
    "</div>\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook shows some examples of how to use [Vertex AI](https://cloud.google.com/vertex-ai/docs) for training a machine learning model. \n",
    "\n",
    "Notebook `02_1_vertex_ai_pcam` showed how to define and submit a **model training job**; then how to upload and deploy that model for serving; and then how to send prediction requests to the deployed model's *Endpoint*. It also showed how to create and use a Managed Tensorboard instance during training, and how to log information about the training run to the Vertex Experiments API.\n",
    "\n",
    "This notebook shows how to set up a [**hyperparameter tuning**](https://en.wikipedia.org/wiki/Hyperparameter_optimization) job using that same model; and how to set up and run a **distributed multi-node training** job.\n",
    "\n",
    "Then, a following set of notebooks show how to use Vertex Pipelines to define an ML workflow for data preprocessing, training, model evaluation, and deployment.\n",
    "\n",
    "The example code is here: https://github.com/verily-src/terra-solutions-ml.\n",
    "\n",
    "### Estimated cost of running this notebook\n",
    "\n",
    "The dataset used for the examples in this notebook is fairly large, as is the base model architecture, and training using the notebook's default configurations will take a number of hours. Time estimates are added in each section.\n",
    "\n",
    "The model training works best with GPU(s)— it runs fine using only CPUs, but training will take an even longer time. For this example, the notebook itself doesn't need GPUs; instead they'll be used by Vertex AI.\n",
    "\n",
    "The HP tuning example should cost ~37USD in Vertex AI charges to run (billed to your ['native' GCP project](https://support.terra.bio/hc/en-us/articles/360051229072-Accessing-advanced-GCP-features-in-Terra)), and the distributed training example < 7.5USD in Vertex AI charges, not including the cost of the notebook instance.\n",
    "\n",
    "### Running on a [Terra](http://app.terra.bio) notebook\n",
    "\n",
    "This example requires that TensorFlow >= 2.6 be installed, and does not require GPUs; instead the example uses GPUs on Vertex AI Training.\n",
    "You can use the default GATK image.\n",
    "\n",
    "You will need to use a ['native' GCP project](https://support.terra.bio/hc/en-us/articles/360051229072-Accessing-advanced-GCP-features-in-Terra) to connect to the Vertex AI services.  The `00_pcam_setup.ipynb` notebook, which should be run before this one, will walk you through that setup.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "If you like, you can shut down the notebook instance/Cloud Environment while the training job runs— monitoring its progress in the Cloud Console UI— and then restart the notebook instance when the job is finished to complete the example. If you do this, you'll need to rerun the import and config cells at the start of the notebook before proceeding.\n",
    "</div>\n",
    "\n",
    "To monitor the logs for a training job while it is running, click on the links output to the notebook when you start the training job.  You can also visit the [Vertex AI tab in the Cloud Console](https://console.cloud.google.com/vertex-ai/training/custom-jobs) for your 'native' GCP project, and click on 'Training', then 'CUSTOM JOBS'.  From that list of jobs, click in to any of them— look for your username— then click on the 'Logs' link in the detailed view.\n",
    "<img src=\"https://storage.googleapis.com/amy-jo/images/terra/CleanShot%202022-02-18%20at%2013.53.16%402x.png\" width=\"90%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73753f168791"
   },
   "source": [
    "### About the ML task and dataset\n",
    "\n",
    "This notebook shows an example of training an _image classification_ [Keras](https://keras.io/) model.\n",
    "\n",
    "The [PatchCamelyon benchmark](https://www.tensorflow.org/datasets/catalog/patch_camelyon) consists of 327.680 color images (96 x 96px) extracted from histopathologic scans of lymph node sections. Each image is annotated with a\n",
    "binary label indicating presence of metastatic tissue. \n",
    "\n",
    "The model uses one of Keras' prebuilt model architectures, [Xception](https://keras.io/api/applications/xception/). The training does [_transfer learning_](https://en.wikipedia.org/wiki/Transfer_learning) , bootstrapping from model weights trained on the ['imagenet'](https://en.wikipedia.org/wiki/ImageNet) dataset.\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/tfds-data/visualization/fig/patch_camelyon-2.0.0.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "af1424701a34"
   },
   "source": [
    "## Config and setup\n",
    "\n",
    "We'll first do some configuration and set some variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fe2863ee9515"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import gapic as aip\n",
    "\n",
    "IMAGE_HEIGHT = 96\n",
    "IMAGE_WIDTH = 96\n",
    "\n",
    "IMAGE_SIZE = (IMAGE_HEIGHT, IMAGE_WIDTH)\n",
    "\n",
    "LABELS = [\"non_metastatic\", \"metastatic\"]\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NB_NUM = \"02-2\"\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3dda6783e1f"
   },
   "source": [
    "We'll set some variables using Workspace Data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d705ee03362a"
   },
   "outputs": [],
   "source": [
    "OWNER_EMAIL = \"\"\n",
    "USER = \"\"\n",
    "\n",
    "if (\n",
    "    \"GOOGLE_PROJECT\" in os.environ\n",
    "):  # This env var is set when running in a Terra workspace\n",
    "    from firecloud import api as fapi\n",
    "\n",
    "    WORKSPACE_NAME = os.environ[\"WORKSPACE_NAME\"]\n",
    "    WORKSPACE_NAMESPACE = os.environ[\"WORKSPACE_NAMESPACE\"]\n",
    "    OWNER_EMAIL = os.environ[\"OWNER_EMAIL\"]\n",
    "    # WORKSPACE_ATTRIBUTES contains key-value pairs from the \"Workspace Data\" section of the Workspace \"Data\" tab.\n",
    "    WORKSPACE_ATTRIBUTES = (\n",
    "        fapi.get_workspace(WORKSPACE_NAMESPACE, WORKSPACE_NAME)\n",
    "        .json()\n",
    "        .get(\"workspace\", {})\n",
    "        .get(\"attributes\", {})\n",
    "    )\n",
    "\n",
    "    # set a variable from the workspace attributes\n",
    "    PYTHON_PACKAGE_GCS_URI_WS = WORKSPACE_ATTRIBUTES[\"PYTHON_PACKAGE_GCS_URI_WS\"]\n",
    "    print(f\"PYTHON_PACKAGE_GCS_URI_WS: {PYTHON_PACKAGE_GCS_URI_WS}\")\n",
    "else:\n",
    "    print(\n",
    "        \"Not running on Terra: you will need to set some variables manually. See below.\"\n",
    "    )\n",
    "\n",
    "if OWNER_EMAIL:\n",
    "    USER = OWNER_EMAIL.split(\"@\")[0].replace('.','-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set some variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Edit the cell below before running it**.  **Replace the values with the ones for your 'native' GCP project** generated when running the `00_pcam_setup.ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7636e20d86eb"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"your-project-id\"\n",
    "# The service account you've set up for these Vertex AI examples\n",
    "TRAINING_SA = \"your-sa-name@your-project-id.iam.gserviceaccount.com\"\n",
    "BUCKET_NAME = (\n",
    "    \"your-bucket-name\"  # don't include the 'gs://' prefix; that is added below\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ae298449f2ef"
   },
   "source": [
    "The `USER` value will be used to create Vertex resource and job names, so that you can locate your info more easily in the GCP Cloud Console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "926b6df936e2"
   },
   "outputs": [],
   "source": [
    "if USER == \"\" or USER is None:\n",
    "    USER = \"your-username\"  # <-- CHANGE THIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44da8bbd7258"
   },
   "source": [
    "Make sure `USER` was set correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ac6fe645f23"
   },
   "outputs": [],
   "source": [
    "print(f\"USER: {USER}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d598bcdd2f4e"
   },
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52e9ea8b0943"
   },
   "source": [
    "### Ensure that the PROJECT_ID is set correctly and set your region\n",
    "\n",
    "Ensure that your project ID has been set correctly. This should be the project ID of the ['native' GCP project](https://support.terra.bio/hc/en-us/articles/360051229072-Accessing-advanced-GCP-features-in-Terra).  (This is different from the project for your workspace)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2554b2b199b1"
   },
   "outputs": [],
   "source": [
    "print(PROJECT_ID)\n",
    "LOCATION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9923bc2d3058"
   },
   "source": [
    "### Check the service account used for some of the Vertex AI calls\n",
    "\n",
    "You'll use the service account that you set up in your native GCP project. Ensure that it's set properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "08e86c7a9fbf"
   },
   "outputs": [],
   "source": [
    "TRAINING_SA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37d5e1007997"
   },
   "source": [
    "### Set a Cloud Storage bucket to use for this example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ee3df410af6b"
   },
   "outputs": [],
   "source": [
    "BUCKET = f\"gs://{BUCKET_NAME}\"\n",
    "print(BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the Python package with the training code to your bucket. This is necessary because the package needs to be in a GCS bucket accessible to Vertex AI in your 'native' GCP project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PYTHON_PACKAGE_GCS_URI = BUCKET + \"/pcam/dist/trainer-0.7.tar.gz\"\n",
    "print(PYTHON_PACKAGE_GCS_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp $PYTHON_PACKAGE_GCS_URI_WS $PYTHON_PACKAGE_GCS_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls $PYTHON_PACKAGE_GCS_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a5d1293a02c6"
   },
   "source": [
    "### Initialize the Vertex AI SDK with your project, location, and bucket settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "58ef2c850623"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=LOCATION, staging_bucket=BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46df5799856d"
   },
   "source": [
    "## Optional: Create an Experiment for tracking training related metadata\n",
    "\n",
    "The Vertex AI Experiments API is useful for tracking information about your training runs.  You can retrieve the logged information via a pandas dataframe for analysis and comparison.\n",
    "\n",
    "We'll start by creating an `Experiment`.  Then, in following sections, we'll define Experiment `runs` and log information about the training jobs to them.\n",
    "\n",
    "**Note**: if you want to log to the same `Experiment` as used in notebook `01_1_vertex_ai`, **find the `EXPERIMENT_NAME` generated in that notebook** and set it here before running the next cell. This doesn't impact how the examples run, but lets you see how you can log multiple _runs_ to the same Experiment, and compare and analyze them in aggregate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d5e3b8885b3c"
   },
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = f\"{USER}-pcam-{NB_NUM}-{TIMESTAMP}\"\n",
    "print(f\"experiment name: {EXPERIMENT_NAME}\")\n",
    "aiplatform.init(experiment=EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ddf6d5c15d0"
   },
   "source": [
    "## Run a hyperperameter tuning job using Vertex AI\n",
    "\n",
    "Next, we'll show how to run a hyperparameter tuning job on Vertex AI, using our training code.  We define the parameters that we want to vary during the HP search— in this example, learning rate and batch size— and how the HP tuning algorithm should vary them during its search.  The training code must accept those parameters as input arguments.\n",
    "\n",
    "You can find more info on hyperparameter tuning [here](https://cloud.google.com/ai-platform-unified/docs/training/using-hyperparameter-tuning).\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Using the default config, the HP tuning job will take about 7 hours to run.</b>\n",
    "</div>\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/amy-jo/images/vertex/hptune.png\" width=\"90%\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "36e009bc5627"
   },
   "outputs": [],
   "source": [
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b375d1e7c4d8"
   },
   "outputs": [],
   "source": [
    "TRAIN_IMAGE = \"us-docker.pkg.dev/vertex-ai/training/tf-gpu.2-6:latest\"\n",
    "EPOCHS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "49bebb556282"
   },
   "outputs": [],
   "source": [
    "ts = int(time.time())\n",
    "\n",
    "MODEL_DISPLAY_NAME = f\"{USER}-pcam_hptune{NB_NUM}-{ts}\"\n",
    "\n",
    "EPOCHS = 2\n",
    "GCS_WORKDIR = f\"gs://{BUCKET_NAME}/{MODEL_DISPLAY_NAME}\"\n",
    "\n",
    "HPT_GCS_MODEL_SAVEDIR = f\"{GCS_WORKDIR}/{ts}\"\n",
    "GCS_METRICS_PATH = f\"/gcs/{BUCKET_NAME}/{MODEL_DISPLAY_NAME}/metrics/{ts}\"\n",
    "print(f\"model savedir: {HPT_GCS_MODEL_SAVEDIR}, GCS_METRICS_PATH: {GCS_METRICS_PATH}\")\n",
    "\n",
    "CMDARGS = [\n",
    "    \"--epochs\",\n",
    "    str(EPOCHS),\n",
    "    # \"--copy-data\",\n",
    "    \"--gcs-workdir\",\n",
    "    GCS_WORKDIR,\n",
    "    \"--gcs-model-savedir\",\n",
    "    HPT_GCS_MODEL_SAVEDIR,\n",
    "    \"--gcs-metrics-path\",\n",
    "    GCS_METRICS_PATH,\n",
    "    \"--image-height\",\n",
    "    str(IMAGE_HEIGHT),\n",
    "    \"--image-width\",\n",
    "    str(IMAGE_WIDTH),\n",
    "    \"--ml-task\",\n",
    "    \"patchcamelyon\",\n",
    "    \"--fine-tune\",\n",
    "    \"false\",\n",
    "    \"--batch-size\",\n",
    "    \"32\",\n",
    "    \"--hptune\",\n",
    "]\n",
    "print(CMDARGS)\n",
    "print(PYTHON_PACKAGE_GCS_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5a357d676c94"
   },
   "source": [
    "Now we'll define the specs for the worker pool of nodes, each of which will run a training job, and the dictionary of hyperparams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8193152ccd3e"
   },
   "outputs": [],
   "source": [
    "worker_pool_specs = [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"n1-highmem-16\",\n",
    "            \"accelerator_type\": \"NVIDIA_TESLA_T4\",\n",
    "            \"accelerator_count\": 1,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"python_package_spec\": {\n",
    "            \"executor_image_uri\": TRAIN_IMAGE,\n",
    "            \"package_uris\": [PYTHON_PACKAGE_GCS_URI],\n",
    "            \"python_module\": \"trainer.task\",\n",
    "            \"args\": CMDARGS,\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "pdict = {\n",
    "    \"batch-size\": hpt.DiscreteParameterSpec(\n",
    "        values=[16, 32, 64, 128, 256], scale=\"linear\"\n",
    "    ),\n",
    "    \"lr\": hpt.DiscreteParameterSpec(values=[1e-4, 1e-3, 1e-2, 1e-1], scale=\"log\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0e48009b285d"
   },
   "source": [
    "Then, we'll create and run a job using that config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8497bf412519"
   },
   "outputs": [],
   "source": [
    "custom_job = aiplatform.CustomJob(\n",
    "    display_name=MODEL_DISPLAY_NAME,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    staging_bucket=BUCKET,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c73f4e19b5af"
   },
   "outputs": [],
   "source": [
    "# Create and run a HyperparameterTuningJob\n",
    "\n",
    "hp_job = aiplatform.HyperparameterTuningJob(\n",
    "    display_name=MODEL_DISPLAY_NAME,\n",
    "    custom_job=custom_job,\n",
    "    metric_spec={\"accuracy\": \"maximize\"},\n",
    "    parameter_spec=pdict,\n",
    "    max_trial_count=32,\n",
    "    parallel_trial_count=4,\n",
    "    search_algorithm=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8921618e6244"
   },
   "outputs": [],
   "source": [
    "# ensure your service account is set correctly\n",
    "TRAINING_SA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ca8d3ee82b28"
   },
   "outputs": [],
   "source": [
    "hp_job.run(\n",
    "    sync=False,\n",
    "    service_account=TRAINING_SA,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61d046b26b97"
   },
   "source": [
    "## Run a multi-node distributed training job using Vertex AI\n",
    "\n",
    "\n",
    "In notebook `02_1_vertex_ai`, we ran a single-node, multi-GPU, distributed training job.\n",
    "Here, we'll show how to run a multi-node distributed training job, on a cluster that Vertex AI sets up for you.  \n",
    "\n",
    "For this example, this training job will run more slowly than the single-node distributed example above, due to greater network latency, especially since we're setting up each node to use just one GPU. However, for larger jobs it can often make sense to distribute the training across multiple nodes.\n",
    "\n",
    "The training run for this section will take ~1.5 hours using the default config."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8349dbd9376d"
   },
   "source": [
    "Log a new 'run' in the 'Experiment' that we set up earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "155de18ee347"
   },
   "outputs": [],
   "source": [
    "aiplatform.start_run(\"run-multinode-distrib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74f437abd78b"
   },
   "source": [
    "Define some variables that will help us define the training job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bcdbe8099d14"
   },
   "source": [
    "Set the path to the training input data, which is in a GCS bucket, using [GCSFuse](https://cloud.google.com/blog/products/ai-machine-learning/cloud-storage-file-system-ai-training) syntax. We'll pass this path as an input to the training code. This will allow the training job to treat the input data directories as if they are on a local file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d7426865527f"
   },
   "outputs": [],
   "source": [
    "ts = int(time.time())\n",
    "\n",
    "MODEL_DISPLAY_NAME = f\"{USER}-pcam_distrib_mw{NB_NUM}-{ts}\"\n",
    "\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 32\n",
    "GCS_WORKDIR = f\"gs://{BUCKET_NAME}/{MODEL_DISPLAY_NAME}\"\n",
    "\n",
    "DISTRIB_GCS_MODEL_SAVEDIR = f\"{GCS_WORKDIR}/{ts}\"\n",
    "# DISTRIB_GCS_MODEL_SAVEDIR = 'AIP_MODEL_DIR' # indicate to use Vertex AI-generated dir\n",
    "\n",
    "DISTRIB_GCS_METRICS_PATH = f\"/gcs/{BUCKET_NAME}/{MODEL_DISPLAY_NAME}/metrics/{ts}\"\n",
    "print(\n",
    "    f\"model savedir: {DISTRIB_GCS_MODEL_SAVEDIR}, DISTRIB_GCS_METRICS_PATH: {DISTRIB_GCS_METRICS_PATH}\"\n",
    ")\n",
    "\n",
    "CMDARGS = [\n",
    "    \"--epochs\",\n",
    "    str(EPOCHS),\n",
    "    \"--batch-size\",\n",
    "    str(BATCH_SIZE),\n",
    "    # \"--copy-data\",\n",
    "    \"--multi-node\",\n",
    "    \"--gcs-workdir\",\n",
    "    GCS_WORKDIR,\n",
    "    \"--gcs-model-savedir\",\n",
    "    DISTRIB_GCS_MODEL_SAVEDIR,\n",
    "    \"--gcs-metrics-path\",\n",
    "    DISTRIB_GCS_METRICS_PATH,\n",
    "    \"--image-height\",\n",
    "    str(IMAGE_HEIGHT),\n",
    "    \"--image-width\",\n",
    "    str(IMAGE_WIDTH),\n",
    "    \"--ml-task\",\n",
    "    \"patchcamelyon\",\n",
    "]\n",
    "print(CMDARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a7275c232f39"
   },
   "outputs": [],
   "source": [
    "argslist = CMDARGS.copy()\n",
    "argslist.insert(argslist.index(\"--multi-node\") + 1, \"True\")\n",
    "args_dict = {argslist[i]: argslist[i + 1] for i in range(0, len(argslist), 2)}\n",
    "print(args_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "add1012e1206"
   },
   "outputs": [],
   "source": [
    "PARAMS = {\"model_display_name\": MODEL_DISPLAY_NAME}\n",
    "PARAMS = {**PARAMS, **args_dict}\n",
    "print(PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ed0ad69a37a"
   },
   "outputs": [],
   "source": [
    "aiplatform.log_params(PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dfa55d7e7624"
   },
   "outputs": [],
   "source": [
    "TRAIN_GPU, TRAIN_NGPU = (aip.AcceleratorType.NVIDIA_TESLA_T4, 2)\n",
    "TRAIN_COMPUTE = \"n1-highmem-16\"\n",
    "TRAIN_IMAGE = \"us-docker.pkg.dev/vertex-ai/training/tf-gpu.2-6:latest\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7d7427dfa9de"
   },
   "source": [
    "This job uses a 'package', not a script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "578e777311fc"
   },
   "outputs": [],
   "source": [
    "PYTHON_PACKAGE_GCS_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "87268b5e316a"
   },
   "outputs": [],
   "source": [
    "job = aiplatform.CustomPythonPackageTrainingJob(\n",
    "    display_name=MODEL_DISPLAY_NAME,\n",
    "    python_package_gcs_uri=PYTHON_PACKAGE_GCS_URI,\n",
    "    python_module_name=\"trainer.task\",\n",
    "    container_uri=TRAIN_IMAGE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3128079fe100"
   },
   "outputs": [],
   "source": [
    "model2 = job.run(\n",
    "    args=CMDARGS,\n",
    "    replica_count=3,\n",
    "    machine_type=TRAIN_COMPUTE,\n",
    "    accelerator_type=TRAIN_GPU.name,\n",
    "    accelerator_count=TRAIN_NGPU,\n",
    "    sync=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6152e4a953a"
   },
   "source": [
    "For this job, the model is just saved to GCS (and not automatically uploaded to Vertex AI).  You can find it here: `DISTRIB_GCS_MODEL_SAVEDIR`.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "Occasionally, you may see this job fail due to issues with an incomplete download of the training dataset from the TensorFlow data hub.  The error in the logs will show: \n",
    "<code>NonMatchingChecksumError(msg) tensorflow_datasets.core.download.download_manager.NonMatchingChecksumError: Artifact</code>. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "165c1258846e"
   },
   "source": [
    "#### Retrieve and save the training metrics to the Experiments `run` info\n",
    "\n",
    "After the training job finishes, you can download and log the metrics information to the Experiment `run`.\n",
    "\n",
    "**Wait until training has completed** to run this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ef25015c741b"
   },
   "outputs": [],
   "source": [
    "DISTRIB_GCS_METRICS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "62fdfee38162"
   },
   "outputs": [],
   "source": [
    "metrics_file = f\"{DISTRIB_GCS_METRICS_PATH}/metrics.json\".replace(\"/gcs/\", \"gs://\")\n",
    "metrics_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e7a653df3d50"
   },
   "outputs": [],
   "source": [
    "!gsutil cat $metrics_file > temp_metrics.json\n",
    "fp = open(\"temp_metrics.json\")\n",
    "metrics = json.load(fp)\n",
    "_ = metrics.pop(\"all_labels\")\n",
    "_ = metrics.pop(\"all_preds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d6185581a93b"
   },
   "outputs": [],
   "source": [
    "metrics = {k: float(v) for k, v in metrics.items()}\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "df4ae31cc2c1"
   },
   "source": [
    "Ensure we're using the correct 'run' context within the Experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cd764726739e"
   },
   "outputs": [],
   "source": [
    "aiplatform.start_run(\"run-multinode-distrib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49657d5f61b6"
   },
   "source": [
    "Log the metrics info to the run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "873c892534b4"
   },
   "outputs": [],
   "source": [
    "aiplatform.log_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "98498839d45b"
   },
   "outputs": [],
   "source": [
    "dataframe = aiplatform.get_experiment_df(experiment=EXPERIMENT_NAME)\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34935bbf18cb"
   },
   "source": [
    "## Cleanup\n",
    "\n",
    "For the examples in this notebook, the model was not automatically uploaded to Vertex AI, so you don't need to delete it.\n",
    "\n",
    "The training instances are automatically torn down after the job completes. \n",
    "\n",
    "If the GCS bucket that you used is not set to automatically delete old files, then you can clean up your GCS bucket as well.  An easy way to do this is via the [Cloud Console UI](https://pantheon.corp.google.com/storage/browser).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "26b0b8a06f5c"
   },
   "outputs": [],
   "source": [
    "# Delete the Experiment\n",
    "# This code requires google-cloud-aiplatform >=1.8\n",
    "c = aiplatform.metadata._Context(EXPERIMENT_NAME)\n",
    "c.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 freeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0399c304611d"
   },
   "source": [
    "--------------------------------\n",
    "Copyright 2021 Verily Life Sciences LLC\n",
    "\n",
    "Use of this source code is governed by a BSD-style  \n",
    "license that can be found in the LICENSE file or at  \n",
    "https://developers.google.com/open-source/licenses/bsd"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "02_2_vertex_ai_pcam.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-7.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-7:m87"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
