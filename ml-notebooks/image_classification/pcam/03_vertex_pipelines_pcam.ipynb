{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f54151188943"
   },
   "source": [
    "# An Image Classification ML workflow using Vertex Pipelines\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "Run the <a href=\"xxx\"><code>00_pcam_setup.ipynb notebook</code></a> first, before running this one.  You'll need the settings info from that notebook.\n",
    "</div>\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook shows how to use [Vertex Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines), with the [KFP SDK](https://www.kubeflow.org/docs/components/pipelines/sdk/sdk-overview/), to define an ML workflow to support an image classification task.\n",
    "\n",
    "The pipeline has the following steps: \n",
    " - trains a model (creating a Managed Tensorboard 'experiment'); \n",
    " - evaluates the model and renders some visualization; \n",
    " - conditionally uploads the trained model if it is sufficiently accurate; \n",
    " - creates an *Endpoint*; and \n",
    " - deploys the model to the endpoint.\n",
    "\n",
    "The eval/visualization step is implemented via a *custom component*, and the the other pipeline steps are implemented via 'prebuilt' components that allow easy access to Vertex AI services. A `.yaml` file is created for the custom 'eval' component, so that it can be used in other pipelines and shared with others.\n",
    "\n",
    "The example code is here: https://github.com/verily-src/terra-solutions-ml.\n",
    "\n",
    "### Estimated cost of running this notebook\n",
    "\n",
    "Using the defaults, this pipeline will take about 2 hours to run.\n",
    "\n",
    "This example should cost < $2.5 in Vertex AI charges to run (billed to your ['native' GCP project](https://support.terra.bio/hc/en-us/articles/360051229072-Accessing-advanced-GCP-features-in-Terra)), not including the cost of the notebook instance.\n",
    "\n",
    "### Running on a [Terra](http://app.terra.bio) notebook\n",
    "\n",
    "This example requires that TensorFlow >= 2.6 be installed, and does not require GPUs; instead the example uses GPUs on Vertex AI Training. You can use the default GATK image.\n",
    "<!-- Currently, you will need to use this image for the Cloud Environment: `gcr.io/ukb-itt-demo-data/amyu_gatk-kfp:v3`. -->\n",
    "\n",
    "You will need to use a ['native' GCP project](https://support.terra.bio/hc/en-us/articles/360051229072-Accessing-advanced-GCP-features-in-Terra) to connect to the Vertex AI services. The `00_pcam_setup.ipynb` notebook, which should be run before this one, will walk you through that setup.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "If you like, you can shut down the notebook instance/Cloud Environment while the pipeline runs— monitoring its progress in the Cloud Console UI— and then restart the notebook instance when the job is finished to complete the example. If you do this, you'll need to rerun the import and config cells at the start of the notebook before proceeding.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98a1f8fafb39"
   },
   "source": [
    "<img src=\"https://storage.googleapis.com/amy-jo/images/terra/pcam_pipeline.png\" width=\"90%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fc31172deff7"
   },
   "source": [
    "### About the ML task and dataset\n",
    "\n",
    "This notebook shows an example of training an _image classification_ [Keras](https://keras.io/) model.\n",
    "\n",
    "The [PatchCamelyon benchmark](https://www.tensorflow.org/datasets/catalog/patch_camelyon) consists of 327.680 color images (96 x 96px) extracted from histopathologic scans of lymph node sections. Each image is annotated with a\n",
    "binary label indicating presence of metastatic tissue. \n",
    "\n",
    "The model uses one of Keras' prebuilt model architectures, [Xception](https://keras.io/api/applications/xception/). The training does [_transfer learning_](https://en.wikipedia.org/wiki/Transfer_learning) , bootstrapping from model weights trained on the ['imagenet'](https://en.wikipedia.org/wiki/ImageNet) dataset.\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/tfds-data/visualization/fig/patch_camelyon-2.0.0.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d068cf50af31"
   },
   "source": [
    "## Config and setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cad750478388"
   },
   "source": [
    "Do some installations and then restart the notebook kernel, then do some imports and define some variables. \n",
    "\n",
    "We're installing both the [KFP SDK](https://www.kubeflow.org/docs/components/pipelines/sdk/sdk-overview/) (to define our pipeline) and a library of Vertex AI Pipelines _components_ that we'll use for some of the steps in our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "085b7be23ea5"
   },
   "outputs": [],
   "source": [
    "# install the first-party components and kfp sdks\n",
    "!unset PIP_TARGET ; pip install --user -U google_cloud_pipeline_components kfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4e9d3bbd9720"
   },
   "outputs": [],
   "source": [
    "# then kernel restart...\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e0324de11526"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from typing import NamedTuple\n",
    "\n",
    "import IPython\n",
    "import numpy as np\n",
    "import PIL\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import gapic\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from kfp.v2 import compiler as v2compiler\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import ClassificationMetrics, Metrics, Output, component\n",
    "from PIL import Image\n",
    "\n",
    "IMAGE_HEIGHT = 96\n",
    "IMAGE_WIDTH = 96\n",
    "NB_NUM = \"03\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3dda6783e1f"
   },
   "source": [
    "We'll set some variables using Workspace Data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d705ee03362a"
   },
   "outputs": [],
   "source": [
    "OWNER_EMAIL = \"\"\n",
    "USER = \"\"\n",
    "\n",
    "if (\n",
    "    \"GOOGLE_PROJECT\" in os.environ\n",
    "):  # This env var is set when running in a Terra workspace\n",
    "    from firecloud import api as fapi\n",
    "\n",
    "    WORKSPACE_NAME = os.environ[\"WORKSPACE_NAME\"]\n",
    "    WORKSPACE_NAMESPACE = os.environ[\"WORKSPACE_NAMESPACE\"]\n",
    "    OWNER_EMAIL = os.environ[\"OWNER_EMAIL\"]\n",
    "    # WORKSPACE_ATTRIBUTES contains key-value pairs from the \"Workspace Data\" section of the Workspace \"Data\" tab.\n",
    "    WORKSPACE_ATTRIBUTES = (\n",
    "        fapi.get_workspace(WORKSPACE_NAMESPACE, WORKSPACE_NAME)\n",
    "        .json()\n",
    "        .get(\"workspace\", {})\n",
    "        .get(\"attributes\", {})\n",
    "    )\n",
    "\n",
    "    # set a variable from the workspace attributes\n",
    "    PYTHON_PACKAGE_GCS_URI_WS = WORKSPACE_ATTRIBUTES[\"PYTHON_PACKAGE_GCS_URI_WS\"]\n",
    "    print(f\"PYTHON_PACKAGE_GCS_URI_WS: {PYTHON_PACKAGE_GCS_URI_WS}\")\n",
    "else:\n",
    "    print(\n",
    "        \"Not running on Terra: you will need to set some variables manually. See below.\"\n",
    "    )\n",
    "\n",
    "if OWNER_EMAIL:\n",
    "    USER = OWNER_EMAIL.split(\"@\")[0].replace('.','-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set some variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Edit the cell below before running it**.  **Replace the values with the ones for your 'native' GCP project** generated when running the `00_pcam_setup.ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7636e20d86eb"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"your-project-id\"\n",
    "# The service account you've set up for these Vertex AI examples\n",
    "TRAINING_SA = \"your-sa-name@your-project-id.iam.gserviceaccount.com\"\n",
    "BUCKET_NAME = (\n",
    "    \"your-bucket-name\"  # don't include the 'gs://' prefix; that is added below\n",
    ")\n",
    "# The TensorBoard instance you created: optional but useful\n",
    "TENSORBOARD_INSTANCE = (\n",
    "    \"projects/xxxxxxxxxxxx/locations/us-central1/tensorboards/xxxxxxxxxxxxxxxxxxx\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ae298449f2ef"
   },
   "source": [
    "The `USER` value will be used to create Vertex resource and job names, so that you can locate your info more easily in the GCP Cloud Console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "926b6df936e2"
   },
   "outputs": [],
   "source": [
    "if USER == \"\" or USER is None:\n",
    "    USER = \"your-username\"  # <-- CHANGE THIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44da8bbd7258"
   },
   "source": [
    "Make sure `USER` was set correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ac6fe645f23"
   },
   "outputs": [],
   "source": [
    "print(f\"USER: {USER}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52e9ea8b0943"
   },
   "source": [
    "### Ensure that the PROJECT_ID is set correctly and set your region\n",
    "\n",
    "Ensure that your project ID has been set correctly. This should be the project ID of the ['native' GCP project](https://support.terra.bio/hc/en-us/articles/360051229072-Accessing-advanced-GCP-features-in-Terra).  (This is different from the project for your workspace)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2554b2b199b1"
   },
   "outputs": [],
   "source": [
    "print(PROJECT_ID)\n",
    "LOCATION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9923bc2d3058"
   },
   "source": [
    "### Check the service account used for some of the Vertex AI calls\n",
    "\n",
    "You'll use the service account that you set up in your native GCP project. Ensure that it's set properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "08e86c7a9fbf"
   },
   "outputs": [],
   "source": [
    "TRAINING_SA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37d5e1007997"
   },
   "source": [
    "### Set a Cloud Storage bucket to use for this example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ee3df410af6b"
   },
   "outputs": [],
   "source": [
    "BUCKET = f\"gs://{BUCKET_NAME}\"\n",
    "print(BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the Python package with the training code to your bucket. This is necessary because the package needs to be in a GCS bucket accessible to Vertex AI in your 'native' GCP project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PYTHON_PACKAGE_GCS_URI = BUCKET + \"/pcam/dist/trainer-0.7.tar.gz\"\n",
    "print(PYTHON_PACKAGE_GCS_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp $PYTHON_PACKAGE_GCS_URI_WS $PYTHON_PACKAGE_GCS_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls $PYTHON_PACKAGE_GCS_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1cfd991e7e60"
   },
   "source": [
    "Next, we'll set the `PIPELINE_ROOT`, used by the Vertex Pipelines job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b4cecce7984e"
   },
   "outputs": [],
   "source": [
    "PATH = %env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin\n",
    "\n",
    "PIPELINE_ROOT = \"{}/pipeline_root/{}\".format(BUCKET, USER)\n",
    "\n",
    "PIPELINE_ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cc98c696556e"
   },
   "source": [
    "Initialize the Vertex AI SDK with your project, location, and bucket information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c100bb26d9d4"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=LOCATION, staging_bucket=BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cdcbb95a2a9f"
   },
   "source": [
    "## 'eval' custom component\n",
    "\n",
    "In this section, we'll create a KFP [custom component](xxx) to generate some metrics visualizations, retrieve the metrics information generated by the training code, and— given threshold information— use that information to determine whether or not the model is accurate enough to deploy.  \n",
    "\n",
    "We'll not only use the component definition in the next section, when we define the pipeline, but we'll also write it to a `yaml` file, `model_eval_pc_component.yaml`.  This allows the component definition to be shared with others, and to be easily resused for other pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7f322cdaf1fc"
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"gcr.io/deeplearning-platform-release/tf2-cpu.2-6:latest\",\n",
    "    output_component_file=\"model_eval_pc_component.yaml\",\n",
    "    # packages_to_install=[\"google-cloud-aiplatform\"],\n",
    ")\n",
    "def classif_model_eval_metrics(\n",
    "    bucket_name: str,\n",
    "    gcs_metrics_path: str,\n",
    "    thresholds_dict_str: str,\n",
    "    metrics: Output[Metrics],\n",
    "    metricsc: Output[ClassificationMetrics],\n",
    ") -> NamedTuple(\"Outputs\", [(\"dep_decision\", str)]):  # Return parameter.\n",
    "\n",
    "    import json\n",
    "\n",
    "    from google.cloud import storage\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    def classification_thresholds_check(\n",
    "        metrics_dict, thresholds_dict, metrics_h, metrics_l\n",
    "    ):\n",
    "        for k, v in thresholds_dict.items():\n",
    "            print(\"k {}, v {}\".format(k, v))\n",
    "            if k in metrics_h:  # higher is better\n",
    "                if metrics_dict[k] < v:  # if under threshold, don't deploy\n",
    "                    print(\n",
    "                        \"{} < {}; threshold check returning False\".format(\n",
    "                            metrics_dict[k], v\n",
    "                        )\n",
    "                    )\n",
    "                    return False\n",
    "            elif k in metrics_l:  # lower is better\n",
    "                if metrics_dict[k] > v:  # if over threshold, don't deploy\n",
    "                    print(\n",
    "                        \"{} > {}; threshold check returning False\".format(\n",
    "                            metrics_dict[k], v\n",
    "                        )\n",
    "                    )\n",
    "                    return False\n",
    "        print(\"threshold checks passed.\")\n",
    "        return True\n",
    "\n",
    "    LABELS = [\"non_metastatic\", \"metastatic\"]\n",
    "\n",
    "    METRICS_H = [\"auc\", \"precision\", \"recall\", \"val_accuracy\"]\n",
    "    METRICS_L = [\"val_loss\"]\n",
    "    METRICS = METRICS_H + METRICS_L\n",
    "\n",
    "    thresholds_dict = json.loads(thresholds_dict_str)\n",
    "\n",
    "    # Fetch model eval info from gcs\n",
    "    gs_metrics_path = f\"{gcs_metrics_path}/metrics.json\".replace(\"/gcs/\", \"\").replace(\n",
    "        f\"{bucket_name}/\", \"\"\n",
    "    )\n",
    "    print(f\"reading from bucket and metrics path: {bucket_name}, {gs_metrics_path}\")\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    blob = bucket.get_blob(gs_metrics_path)\n",
    "\n",
    "    gcs_metrics_str = blob.download_as_string()\n",
    "\n",
    "    print(f\"metrics string: {gcs_metrics_str}\")\n",
    "\n",
    "    metrics_info = json.loads(gcs_metrics_str)\n",
    "    print(f\"got metrics info: {metrics_info}\")\n",
    "    all_labels = json.loads(metrics_info[\"all_labels\"])\n",
    "    all_preds = json.loads(metrics_info[\"all_preds\"])\n",
    "\n",
    "    metricsc.log_confusion_matrix(\n",
    "        LABELS,\n",
    "        confusion_matrix(all_labels, all_preds).tolist(),\n",
    "    )\n",
    "\n",
    "    # log textual metrics info as well\n",
    "    for metric in METRICS:\n",
    "        try:\n",
    "            print(f\"logging {metric} of {metrics_info[metric]}\")\n",
    "            metrics.log_metric(metric, float(metrics_info[metric]))\n",
    "        except KeyError as e:\n",
    "            print(e)\n",
    "\n",
    "    deploy = classification_thresholds_check(\n",
    "        metrics_info, thresholds_dict, METRICS_H, METRICS_L\n",
    "    )\n",
    "\n",
    "    if deploy:\n",
    "        dep_decision = \"true\"\n",
    "    else:\n",
    "        dep_decision = \"false\"\n",
    "    print(f\"deployment decision is {dep_decision}\")\n",
    "\n",
    "    return (dep_decision,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76478fc309b6"
   },
   "source": [
    "## Define the pipeline\n",
    "\n",
    "Now we're ready to define the pipeline. \n",
    "\n",
    "We'll first set some variables for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4f6fd46fbcb8"
   },
   "outputs": [],
   "source": [
    "ts = int(time.time())\n",
    "\n",
    "MODEL_DISPLAY_NAME = f\"{USER}-pcam{NB_NUM}-{ts}\"\n",
    "PIPELINE_NAME = f\"{USER}-keras-patchcamelyon-pipeline\"\n",
    "EPOCHS = 3\n",
    "GCS_WORKDIR = f\"gs://{BUCKET_NAME}/{MODEL_DISPLAY_NAME}\"\n",
    "\n",
    "GCS_MODEL_SAVEDIR = f\"{GCS_WORKDIR}/{ts}\"\n",
    "GCS_METRICS_PATH = f\"/gcs/{BUCKET_NAME}/{MODEL_DISPLAY_NAME}/metrics/{ts}\"\n",
    "print(f\"model savedir: {GCS_MODEL_SAVEDIR}, GCS_METRICS_PATH: {GCS_METRICS_PATH}\")\n",
    "\n",
    "CMDARGS = [\n",
    "    \"--epochs\",\n",
    "    str(EPOCHS),\n",
    "    # \"--copy-data\",\n",
    "    \"--gcs-workdir\",\n",
    "    GCS_WORKDIR,\n",
    "    \"--gcs-model-savedir\",\n",
    "    GCS_MODEL_SAVEDIR,\n",
    "    \"--gcs-metrics-path\",\n",
    "    GCS_METRICS_PATH,\n",
    "    \"--image-height\",\n",
    "    str(IMAGE_HEIGHT),\n",
    "    \"--image-width\",\n",
    "    str(IMAGE_WIDTH),\n",
    "    \"--ml-task\",\n",
    "    \"patchcamelyon\",\n",
    "]\n",
    "CMD_STRING = json.dumps(CMDARGS)\n",
    "print(f\"cmd args : {CMDARGS}\")\n",
    "print(f\"cmd args string: {CMD_STRING}\")\n",
    "\n",
    "THRESHOLD_DICT = {\"val_accuracy\": 0.79}\n",
    "THRESHOLD_DICT_STRING = json.dumps(THRESHOLD_DICT)\n",
    "print(THRESHOLD_DICT_STRING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d61e92df1bed"
   },
   "source": [
    "Now we're ready to define the pipeline itself.  \n",
    "\n",
    "For one of the pipeline steps, we're using the custom component we defined above.  This is the `classif_model_eval_metrics(..)` step.\n",
    "\n",
    "For the other steps, we're using the [google_cloud_pipeline_components](https://github.com/kubeflow/pipelines/tree/master/components/google-cloud/google_cloud_pipeline_components), which give easy access to Vertex AI operations.\n",
    "\n",
    "If you did not set `TENSORBOARD_INSTANCE` above, then **comment out that arg** in the `gcc_aip.CustomPythonPackageTrainingJobRunOp()` method below.\n",
    "\n",
    "We're using a conditional statement to determine whether or not to upload and deploy the trained model given the results of the evaluation step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2d0ae84973ec"
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=PIPELINE_NAME,\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "def image_classification_pl(\n",
    "    project: str = PROJECT_ID,\n",
    "    location: str = LOCATION,\n",
    "    gcs_workdir: str = GCS_WORKDIR,\n",
    "    bucket_name: str = BUCKET_NAME,\n",
    "    model_savedir: str = GCS_MODEL_SAVEDIR,\n",
    "    gcs_metrics_path: str = GCS_METRICS_PATH,\n",
    "    display_name: str = MODEL_DISPLAY_NAME,\n",
    "    python_package_gcs_uri: str = PYTHON_PACKAGE_GCS_URI,\n",
    "    python_module_name: str = \"trainer.task\",\n",
    "    staging_bucket: str = GCS_WORKDIR,\n",
    "    thresholds_dict_str: str = THRESHOLD_DICT_STRING,\n",
    "    accelerator_count: int = 2,\n",
    "    machine_type: str = \"n1-highmem-8\",\n",
    "    accelerator_type: str = gapic.AcceleratorType.NVIDIA_TESLA_T4.name,\n",
    "    boot_disk_size_gb: int = 256,\n",
    "    container_uri: str = \"us-docker.pkg.dev/vertex-ai/training/tf-gpu.2-6:latest\",\n",
    "    serving_container_image_uri: str = \"us-docker.pkg.dev/cloud-aiplatform/prediction/tf2-cpu.2-6:latest\",\n",
    "):\n",
    "    training_job_run_op = gcc_aip.CustomPythonPackageTrainingJobRunOp(\n",
    "        container_uri=container_uri,\n",
    "        python_package_gcs_uri=python_package_gcs_uri,\n",
    "        python_module_name=python_module_name,\n",
    "        staging_bucket=staging_bucket,\n",
    "        project=project,\n",
    "        location=location,\n",
    "        display_name=display_name,\n",
    "        accelerator_count=accelerator_count,\n",
    "        accelerator_type=accelerator_type,\n",
    "        machine_type=machine_type,\n",
    "        boot_disk_size_gb=boot_disk_size_gb,\n",
    "        service_account=TRAINING_SA,\n",
    "        tensorboard=TENSORBOARD_INSTANCE,  # comment out this arg if you did not set TENSORBOARD_INSTANCE\n",
    "        args=CMDARGS,\n",
    "    )\n",
    "\n",
    "    endpoint_create_op = gcc_aip.EndpointCreateOp(\n",
    "        project=project,\n",
    "        display_name=display_name,\n",
    "    )\n",
    "\n",
    "    model_eval_task = classif_model_eval_metrics(\n",
    "        bucket_name, gcs_metrics_path, thresholds_dict_str\n",
    "    )\n",
    "    model_eval_task.after(training_job_run_op)\n",
    "\n",
    "    with dsl.Condition(\n",
    "        model_eval_task.outputs[\"dep_decision\"] == \"true\",\n",
    "        name=\"deploy_decision\",\n",
    "    ):\n",
    "\n",
    "        model_upload_op = gcc_aip.ModelUploadOp(\n",
    "            project=project,\n",
    "            display_name=display_name,\n",
    "            artifact_uri=GCS_MODEL_SAVEDIR,\n",
    "            serving_container_image_uri=serving_container_image_uri,\n",
    "            # serving_container_environment_variables={\"NOT_USED\": \"NO_VALUE\"},\n",
    "        )\n",
    "\n",
    "        model_deploy_op = gcc_aip.ModelDeployOp(  # noqa: F841\n",
    "            endpoint=endpoint_create_op.outputs[\"endpoint\"],\n",
    "            model=model_upload_op.outputs[\"model\"],\n",
    "            deployed_model_display_name=display_name,\n",
    "            dedicated_resources_machine_type=\"n1-standard-4\",\n",
    "            dedicated_resources_min_replica_count=1,\n",
    "            dedicated_resources_max_replica_count=2,\n",
    "            traffic_split={\"0\": 100},\n",
    "            # can also specify accelerator type and count.\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6954446288a1"
   },
   "source": [
    "## Compile and run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c69c0e8cc524"
   },
   "outputs": [],
   "source": [
    "# compile the pipeline\n",
    "\n",
    "v2compiler.Compiler().compile(\n",
    "    pipeline_func=image_classification_pl, package_path=\"pcam_pl_spec.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ba81bd9b005c"
   },
   "source": [
    "Run the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "37aec24c7517"
   },
   "outputs": [],
   "source": [
    "job = aiplatform.PipelineJob(\n",
    "    display_name=MODEL_DISPLAY_NAME,\n",
    "    template_path=\"pcam_pl_spec.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={\n",
    "        \"staging_bucket\": GCS_WORKDIR,\n",
    "        \"project\": PROJECT_ID,\n",
    "        \"display_name\": MODEL_DISPLAY_NAME,\n",
    "    },\n",
    ")\n",
    "\n",
    "job.run(sync=False, service_account=TRAINING_SA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you like, you can stop the notebook instance/Terra cloud environment while the pipeline runs, then restart it when it's finished. If you do so, you'll need first to rerun the imports at the start of the notebook before continuing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e21d8d3ea179"
   },
   "source": [
    "### View model metrics\n",
    "\n",
    "You can view the numeric metrics and confusion matrix for the trained model by clicking on the output artifacts of the `classif-model-eval-metrics` step.\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/amy-jo/images/terra/pcam_metrics.png\" width=\"90%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b78e1a3b36d8"
   },
   "source": [
    "## Artifact logging and lineage tracking\n",
    "\n",
    "When you run a pipeline using Vertex AI Pipelines, the artifacts and parameters of your pipeline run are automatically logged to the [Vertex ML Metadata server](https://cloud.google.com/vertex-ai/docs/ml-metadata). This lets you analyze the lineage of your pipelines' step executions and artifacts, and you can view lineage information in the Pipelines UI as well as information about a pipeline's run graph.\n",
    "\n",
    "[View lineage information](https://cloud.google.com/vertex-ai/docs/pipelines/lineage) by clicking on 'VIEW LINEAGE' after selecting an output Artifact of a Pipeline step. \n",
    "\n",
    "<img src=\"https://storage.googleapis.com/amy-jo/images/vertex/view_lineage.png\" width=\"90%\"/>\n",
    "\n",
    "This view lets you see how resources and other artifacts are connected by step executions— in a sense the inversion of the execution graph above.\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/amy-jo/images/vertex/lineage_graph.png\" width=\"70%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65aede3c9398"
   },
   "source": [
    "## Prediction\n",
    "\n",
    "\n",
    "After the pipeline has finished running, we can send prediction requests to the model endpoint that was deployed as part of the pipeline workflow.\n",
    "\n",
    "If you've lost your notebook context, rerun the \"Config and setup\" section before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10d764a57745"
   },
   "outputs": [],
   "source": [
    "LABELS = [\"non_metastatic\", \"metastatic\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first instantiate a a client object for the endpoint resource to which the model is deployed.\n",
    "We can do this by looking for the `USER`, dataset, and notebook number strings in the endpoint's display name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = None\n",
    "epl = aiplatform.Endpoint.list()\n",
    "for ep in epl:\n",
    "    if f\"{USER}-pcam{NB_NUM}\" in ep.display_name:\n",
    "        print(f\"found a match for USER {USER} & pcam: {ep}, {ep.display_name}\")\n",
    "        endpoint = ep\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2df3edd36a20"
   },
   "source": [
    "If the code above did not select the correct endpoint, you can instead uncomment and edit the following cell to use the ID of the Endpoint created by the pipeline run. The endpoint ID can be found via the Pipelines UI in the Cloud Console, by clicking on the Artifact produced by the `endpoint-create` step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "38c909a668d5"
   },
   "outputs": [],
   "source": [
    "# use the ID of the endpoint that was created via the pipeline run.\n",
    "# endpoint = aiplatform.Endpoint(\"xxxxxxxxxxxxxxxxxxx\")  # <-- CHANGE THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm the endpoint is set properly\n",
    "print(endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download an example image to use for the prediction request.  The image below is labeled `non_metastatic`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "143a48a631c1"
   },
   "outputs": [],
   "source": [
    "!gsutil cp gs://fc-b60eeef5-8162-47a8-8114-d8dd82b65653/data/patch_camelyon/label_0/download.png ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9e8472b991c"
   },
   "source": [
    "Resize the image so it matches what model input is expecting, and render it as a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d8deafd49b85"
   },
   "outputs": [],
   "source": [
    "image_file = \"./download.png\"\n",
    "display(IPython.display.Image(image_file))\n",
    "\n",
    "img1 = Image.open(image_file)\n",
    "img2 = img1.resize((92, 92), resample=PIL.Image.NEAREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23467a388837"
   },
   "outputs": [],
   "source": [
    "image_data = np.array(img2)\n",
    "img_array = np.float32(image_data)[:, :, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "092bf260348a"
   },
   "outputs": [],
   "source": [
    "img_array2 = img_array.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbddb71b1f86"
   },
   "source": [
    "Send the image data to the Endpoint where your model was deployed, for online prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41c50f79edf6"
   },
   "outputs": [],
   "source": [
    "predictions = endpoint.predict(instances=[img_array2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ed916eef0b7"
   },
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "95a288b5a5d6"
   },
   "outputs": [],
   "source": [
    "image_predictions = predictions.predictions[0]\n",
    "image_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e7ddbb11dbdf"
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"image is predicted to be: {LABELS[image_predictions.index(max(image_predictions))]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c18cf134b97d"
   },
   "source": [
    "## Leverage Pipelines step caching for a later run\n",
    "\n",
    "Vertex Pipelines [caches](https://cloud.google.com/vertex-ai/docs/pipelines/configure-caching) step execution results. When Vertex AI Pipelines runs a pipeline, it checks to see whether or not an _execution_ exists in Vertex ML Metadata with the interface (cache key) of each pipeline step. If there is a matching execution in Vertex ML Metadata, the outputs of that execution are used and the step is skipped— unless caching has been _disabled_ for that pipeline run.\n",
    "\n",
    "This feature can be very useful for iterative development (among other things).  We can demonstrate this with the example pipeline.  Suppose we decide that we want to change the 'threshold' information that we're using to decide whether or not a model is accurate enough to deploy. We can do this by _cloning_ the pipeline, and changing just the threshold input parameter.  You can do this easily via the Cloud Console. **Don't change the name of the pipeline when you clone it**— caching is only applied across pipelines of the same name.\n",
    "\n",
    "If you took the defaults above, the threshold info is `{'val_accuracy': 0.79}`— try changing that value to 0.89, which will likely be too high for the conditional to pass, so with that change, the model won't be deployed.\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/amy-jo/images/terra/pcam_cloning.png\" width=\"90%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "274f14e9f032"
   },
   "source": [
    "Once the new pipeline run starts up, you can see that the pipeline run is using the cached versions of the training and endpoint creation step executions— whose input parameters did not change.  A cache hit is indicated by the curveed arrow. This saves a lot of development time, particularly for long-running steps such as model training.\n",
    "\n",
    "The 'eval metrics' step, whose inputs _did_ change, is re-run with the new threshold information.\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/amy-jo/images/terra/pcam_caching.png\" width=\"90%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3b6c36fafdad"
   },
   "source": [
    "After the pipeline finishes running, we can see that the higher threshold changed the results— this time, the model was not considered accurate enough to deploy, and so the pipeline conditional did not hold.\n",
    "\n",
    "<!-- <img src=\"https://storage.googleapis.com/amy-jo/images/vertex/new_cloned_run.png\" width=\"90%\"/> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8611b0790b8"
   },
   "source": [
    "## Cleanup\n",
    "\n",
    "If you've lost your notebook context, rerun the \"Config and setup\" section before continuing.\n",
    "\n",
    "Delete the endpoint and model that you created.  The training instances and pipeline step instances are automatically torn down after the job completes. \n",
    "If the GCS bucket that you used is not set to automatically delete old files, then you can clean up your GCS bucket as well.  An easy way to do this is via the [Cloud Console UI](https://pantheon.corp.google.com/storage/browser).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell to delete your model and endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epl = aiplatform.Endpoint.list()\n",
    "for ep in epl:\n",
    "    if f\"{USER}-pcam{NB_NUM}\" in ep.display_name:\n",
    "        print(f\"found a match for USER {USER}: {ep}, {ep.display_name}\")\n",
    "        print(\"models deployed to the endpoint:\")\n",
    "        models = ep.list_models()\n",
    "        print(\"\\nundeploying models from endpoint\")\n",
    "        ep.undeploy_all()\n",
    "        for m in models:\n",
    "            model = aiplatform.Model(m.model)\n",
    "            print(f\"\\ndeleting model: {model}\")\n",
    "            model.delete()\n",
    "        print(f\"\\ndeleting endpoint: {ep}\")\n",
    "        ep.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3001c9a3c62d"
   },
   "source": [
    "In case the code above— which filters for `USER`, dataset, and notebook number— did not catch all your models and endpoints, you can clean them up via the Cloud Console UI, or programmatically as follows. Uncomment the cells below and edit them before running.\n",
    "\n",
    "Before you run the following two cells, **edit them to use the IDs of the Model and Endpoint created by the pipeline run**.   \n",
    "The model ID can be found via the Pipelines UI in the Cloud Console, by clicking on the Artifact produced by the `custompythonpackagetrainingjob-run` step.  \n",
    "The endpoint ID can be found via the Pipelines UI, by clicking on the Artifact produced by the `endpoint-create` step.  If you've already reconstituted the `endpoint` above, for prediction, you don't need to do it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "138b7042398e"
   },
   "outputs": [],
   "source": [
    "# # use the ID of the endpoint that was created via the pipeline run. Create it from its ID\n",
    "# # if you didn't do so above in the Prediction section.\n",
    "# endpoint = aiplatform.Endpoint('xxxxxxxxxxxxxxxxxxx')  # <-- CHANGE THIS\n",
    "# print(endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5711f3732088"
   },
   "outputs": [],
   "source": [
    "# # use the ID of the model that was created via the pipeline run.\n",
    "# model = aiplatform.Model(\"xxxxxxxxxxxxxxxxxxx\")  # <-- CHANGE THIS\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5c1f7dc85017"
   },
   "outputs": [],
   "source": [
    "# endpoint.undeploy_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "12973199b0a3"
   },
   "outputs": [],
   "source": [
    "# # Delete the model\n",
    "# model.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "622b58167a5e"
   },
   "outputs": [],
   "source": [
    "# # Delete the endpoint\n",
    "# endpoint.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 freeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0406f5c0aa53"
   },
   "source": [
    "--------------------------------\n",
    "Copyright 2021 Verily Life Sciences LLC\n",
    "\n",
    "Use of this source code is governed by a BSD-style  \n",
    "license that can be found in the LICENSE file or at  \n",
    "https://developers.google.com/open-source/licenses/bsd"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "03_vertex_pipelines_pcam.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-7.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-7:m87"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
